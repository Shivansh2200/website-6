{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"What is this Guide for? This guide is designed for application developer or a compute infrastructure platform engineer considering their options for Kubernetes networking, with specific focus on Tungsten Fabric Carbide . Kubernetes Cluster Networking functions are central to applications running on top of Kubernetes. These functions include: Network communications between Pods through Services; Network communications between the outside world and externally-facing Services ; and Network policies that provide fine-grained control over what network communication flows are allowed. To do the above, Kubernetes cluster must have a Container Network Interface (\"CNI\") plugin installed. Kubernetes documentation web site lists a number of options , Tungsten Fabric being the one we cover in this document. We will use a sample 3-tier application to go through the three main function areas listed above, and explain what Tungsten Fabric does in each of the cases. Where Tungsten Fabric provides extra functionality beyond Kubernetes baseline, we will also say so. To follow along with our use cases, you should deploy your own copy of a Quick Start of Tungsten Fabric (\"TF\") Carbide with Kubernetes (\"K8s\") on AWS . Prerequisites This guide assumes that you are comfortable with how to: Deploy a CloudFormation template into your AWS account; Connect to an EC2 instance in AWS with an SSH client and SSH private key; Deploy applications to Kubernetes using kubectl CLI tool; Use Linux CLI/terminal tools, like less and nano . Introduction to our sample app To demonstrate how Tungsten Fabric can help us get an application running, accessible from the Internet, and then secured, we will use the mock application called \" yelb \". It was written and is maintained by one of the Developer Advocates at AWS. The reason for choosing this application is that it is simple, well-documented , and is ready to run on Kubernetes . Please see the \" Yelb Architecture \" link for more detail, but at the high level it looks like this: (Original image URL: https://github.com/mreferre/yelb/raw/master/yelb-architecture.png ) The application is made of four Deployments: yelb-ui , yelb-appserver , yelb-db and yelb-cache . Each Deployment is fronted by a respective Kubernetes Service. The yelb-ui Service can also be fronted by Kubernetes Ingress, providing you with L7 HTTP routing. Getting ready For our exercises, we will need to have the following in place: Access to our Kubernetes cluster with Tungsten Fabric through Kubernetes' kubectl CLI tool; and A copy of yelb Access our Kubernetes cluster If you followed the steps in Tungsten Fabric Carbide Quick Start on AWS guide , you should be able to log in into your QuickStart sandbox control node as described in the Accessing the Cluster section of the guide. To find out the public DNS host name of your sandbox control node, look in the Outputs tab of the AWS CloudFormation UI for the template you used to deploy Kubernetes with Tungsten Fabric Carbide: Once on the sandbox control node, run: sudo -s kubectl get nodes which should display output similar to: NAME STATUS ROLES AGE VERSION ip-172-25-1-105.us-west-1.compute.internal NotReady master 1m v1.9.2 ip-172-25-1-146.us-west-1.compute.internal Ready <none> 1m v1.9.2 ip-172-25-1-202.us-west-1.compute.internal Ready <none> 1m v1.9.2 Get a copy of yelb application After you've successfully connected to the sandbox control node and verified that your kubectl works correctly, use the following commands to get a copy of yelb and change your working directory to the one with its Kubernetes manifests (while running as root): # Install git yum -y install git # Clone the Yelb repo and checkout the branch we used for this guide git clone https://github.com/mreferre/yelb cd yelb git checkout 9cba442 # to make sure our examples keep working as yelb evolves # Change to the manifests directory cd deployments/platformdeployment/Kubernetes/yaml/ What's next At this point, you have a functional sandbox Kubernetes cluster with 2 compute nodes and an application that you can use to play around. The rest of this document will walk you through examples of how to deal with a few common scenarios around networking and network security that you can encounter when developing and operating an application that runs on Kubernetes. Each use case is stand-alone, and does not require you to complete any other use cases in this document. Feel free to jump to any one as you see fit: Basic app connectivity through Kubernetes' Services Advanced external app connectivity through Kubernetes' Ingress Coarse application isolation through Kubernetes Namespaces Application micro-segmentation through Kubernetes Network Policies","title":"Home"},{"location":"#what-is-this-guide-for","text":"This guide is designed for application developer or a compute infrastructure platform engineer considering their options for Kubernetes networking, with specific focus on Tungsten Fabric Carbide . Kubernetes Cluster Networking functions are central to applications running on top of Kubernetes. These functions include: Network communications between Pods through Services; Network communications between the outside world and externally-facing Services ; and Network policies that provide fine-grained control over what network communication flows are allowed. To do the above, Kubernetes cluster must have a Container Network Interface (\"CNI\") plugin installed. Kubernetes documentation web site lists a number of options , Tungsten Fabric being the one we cover in this document. We will use a sample 3-tier application to go through the three main function areas listed above, and explain what Tungsten Fabric does in each of the cases. Where Tungsten Fabric provides extra functionality beyond Kubernetes baseline, we will also say so. To follow along with our use cases, you should deploy your own copy of a Quick Start of Tungsten Fabric (\"TF\") Carbide with Kubernetes (\"K8s\") on AWS .","title":"What is this Guide for?"},{"location":"#prerequisites","text":"This guide assumes that you are comfortable with how to: Deploy a CloudFormation template into your AWS account; Connect to an EC2 instance in AWS with an SSH client and SSH private key; Deploy applications to Kubernetes using kubectl CLI tool; Use Linux CLI/terminal tools, like less and nano .","title":"Prerequisites"},{"location":"#introduction-to-our-sample-app","text":"To demonstrate how Tungsten Fabric can help us get an application running, accessible from the Internet, and then secured, we will use the mock application called \" yelb \". It was written and is maintained by one of the Developer Advocates at AWS. The reason for choosing this application is that it is simple, well-documented , and is ready to run on Kubernetes . Please see the \" Yelb Architecture \" link for more detail, but at the high level it looks like this: (Original image URL: https://github.com/mreferre/yelb/raw/master/yelb-architecture.png ) The application is made of four Deployments: yelb-ui , yelb-appserver , yelb-db and yelb-cache . Each Deployment is fronted by a respective Kubernetes Service. The yelb-ui Service can also be fronted by Kubernetes Ingress, providing you with L7 HTTP routing.","title":"Introduction to our sample app"},{"location":"#getting-ready","text":"For our exercises, we will need to have the following in place: Access to our Kubernetes cluster with Tungsten Fabric through Kubernetes' kubectl CLI tool; and A copy of yelb","title":"Getting ready"},{"location":"#access-our-kubernetes-cluster","text":"If you followed the steps in Tungsten Fabric Carbide Quick Start on AWS guide , you should be able to log in into your QuickStart sandbox control node as described in the Accessing the Cluster section of the guide. To find out the public DNS host name of your sandbox control node, look in the Outputs tab of the AWS CloudFormation UI for the template you used to deploy Kubernetes with Tungsten Fabric Carbide: Once on the sandbox control node, run: sudo -s kubectl get nodes which should display output similar to: NAME STATUS ROLES AGE VERSION ip-172-25-1-105.us-west-1.compute.internal NotReady master 1m v1.9.2 ip-172-25-1-146.us-west-1.compute.internal Ready <none> 1m v1.9.2 ip-172-25-1-202.us-west-1.compute.internal Ready <none> 1m v1.9.2","title":"Access our Kubernetes cluster"},{"location":"#get-a-copy-of-yelb-application","text":"After you've successfully connected to the sandbox control node and verified that your kubectl works correctly, use the following commands to get a copy of yelb and change your working directory to the one with its Kubernetes manifests (while running as root): # Install git yum -y install git # Clone the Yelb repo and checkout the branch we used for this guide git clone https://github.com/mreferre/yelb cd yelb git checkout 9cba442 # to make sure our examples keep working as yelb evolves # Change to the manifests directory cd deployments/platformdeployment/Kubernetes/yaml/","title":"Get a copy of yelb application"},{"location":"#whats-next","text":"At this point, you have a functional sandbox Kubernetes cluster with 2 compute nodes and an application that you can use to play around. The rest of this document will walk you through examples of how to deal with a few common scenarios around networking and network security that you can encounter when developing and operating an application that runs on Kubernetes. Each use case is stand-alone, and does not require you to complete any other use cases in this document. Feel free to jump to any one as you see fit: Basic app connectivity through Kubernetes' Services Advanced external app connectivity through Kubernetes' Ingress Coarse application isolation through Kubernetes Namespaces Application micro-segmentation through Kubernetes Network Policies","title":"What's next"},{"location":"use_case_1/","text":"Use case 1: Basic app connectivity through Kubernetes' Services This is the most basic and fundamental functionality that all Kubernetes CNI plugins provide. Application Pods need to be able to talk to each other, and Kubernetes Services are the way to ensure that it is possible with Pods coming and going with time to support application scale and availability. When would I care? All major CNI plugins provide basic Pod to Pod connectivity, as well as some of the Service types, such as ClusterIP . In addition to that, Tungsten Fabric comes with out of the box support of Service type LoadBalancer . When running on AWS, using LoadBalancer Service in your manifest creates a public-facing AWS ELB, making your application available from the Internet in one step. This also means you can use the Kubernetes deployment manifest for your application unchanged in all places where Tungsten Fabric with Kubernetes is available - on premises and in all major public clouds. Deployments When you create a Deployment, a CNI works in concert with Kubernetes to allocate network IP addresses for each of your application Pods, and \"wire up\" each pod to the cluster network. Note: most CNIs work by creating an overlay network that in most cases is contained within the boundaries of a single Kubernetes cluster. As the result, Pods in different clusters cannot communicate directly. While we will not cover multi-cluster scenarios in this document, Tungsten Fabric is capable of supporting such configurations. A single installation of Tungsten Fabric can serve multiple Kubernetes clusters at the same time. In such scenario Pods from different clusters can communicate with each other directly, even if Kubernetes clusters themselves are in different locations. Services Services in Kubernetes are \"an abstract way to expose an application running on a set of Pods\". In most cases, a Service is a simple Round-Robin Load Balancer. It has a Virtual IP address (\"VIP\") for receiving network requests, and a zero or more Endpoint IP addresses that it will forward these requests to. In most cases a Service will automatically discover Endpoint IP addresses that belong to the application Pods by looking for matching labels (called \"Selectors\") on running Pods. Sample app's Deployments and Services Make sure you're on the sandbox control node, logged in as root, and in the correct directory: # Make sure we're root whoami | grep root || sudo -s # Change to the manifests directory cd /home/centos/yelb/deployments/platformdeployment/Kubernetes/yaml Review the cnawebapp-loadbalancer.yaml file, looking for the sections that start with Kind: Deployment and Kind: Service less cnawebapp-loadbalancer.yaml (Use arrows / PgUp / PgDn to navigate; press q to exit) Note that: spec.template.spec.containers.ports.containerPort in Deployments shows what TCP port a Pod will listen to; spec.ports in Services shows what port a Service's VIP will be listening to; spec.selector in Services shows what labels on Pods a Service will look for to send traffic to. Next, let's deploy our app and see what happens: kubectl create -f cnawebapp-loadbalancer.yaml This will create the following application topology: If the application deploys without errors, we should be able to see that: All Pods have their own IP address and are listening on their respective ports: kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE redis-server-5786bc9958-mkm6s 1/1 Running 0 9m 10.47.255.247 ip-172-25-1-190.us-west-1.compute.internal yelb-appserver-7975bf6fb-xvzgh 1/1 Running 0 9m 10.47.255.248 ip-172-25-1-190.us-west-1.compute.internal yelb-db-ccd4bd9f5-9pt9m 1/1 Running 0 9m 10.47.255.249 ip-172-25-1-79.us-west-1.compute.internal yelb-ui-5cd947475f-dsvbd 1/1 Running 0 9m 10.47.255.250 ip-172-25-1-79.us-west-1.compute.internal All Services have a VIP and a port they're listening on: kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 4h redis-server ClusterIP 10.103.111.102 <none> 6379/TCP 9m yelb-appserver ClusterIP 10.109.212.183 <none> 4567/TCP 9m yelb-db ClusterIP 10.96.201.173 <none> 5432/TCP 9m yelb-ui LoadBalancer 10.108.170.55 aa01af9988cc3... 80:32393/TCP 9m All Services have discovered their respective Endpoints: kubectl get ep NAME ENDPOINTS AGE kubernetes 172.25.1.105:6443 4h redis-server 10.47.255.247:6379 10m yelb-appserver 10.47.255.248:4567 10m yelb-db 10.47.255.249:5432 10m yelb-ui 10.47.255.250:80 10m Since Tungsten Fabric provides support for Kubernetes Service type LoadBalancer , we should now be able to connect to our application from the Internet. Let's find out what the public DNS name for our load balancer is: # kubectl get svc yelb-ui -o wide NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR yelb-ui LoadBalancer 10.108.170.55 aa01af9988cc311e9badf06b57ebf630-1452353610.us-west-1.elb.amazonaws.com 80:32393/TCP 11m app=yelb-ui,tier=frontend We can see that our app is available on aa01af9988cc311e9badf06b57ebf630-1452353610.us-west-1.elb.amazonaws.com , so let's check it by pointing our web browser to it: It worked! Cleanup Once you've played with the app for a moment or two, feel free to undeploy it: kubectl delete -f cnawebapp-loadbalancer.yaml Recap and what's next In this Use Case, we've got our sample application deployed and available from the Internet using Tungsten Fabric Kubernetes CNI plugin and integration with AWS' Elastic Load Balancing. Our sample application is on the web. If this is all we wanted - we are done. However, if we need things like SSL offload or want to send incoming requests to different application components based on HTTP host and/or path, we will need to use Kubernetes Ingress. The Use Case 2 covers this scenario.","title":"Use case 1"},{"location":"use_case_1/#use-case-1-basic-app-connectivity-through-kubernetes-services","text":"This is the most basic and fundamental functionality that all Kubernetes CNI plugins provide. Application Pods need to be able to talk to each other, and Kubernetes Services are the way to ensure that it is possible with Pods coming and going with time to support application scale and availability.","title":"Use case 1: Basic app connectivity through Kubernetes' Services"},{"location":"use_case_1/#when-would-i-care","text":"All major CNI plugins provide basic Pod to Pod connectivity, as well as some of the Service types, such as ClusterIP . In addition to that, Tungsten Fabric comes with out of the box support of Service type LoadBalancer . When running on AWS, using LoadBalancer Service in your manifest creates a public-facing AWS ELB, making your application available from the Internet in one step. This also means you can use the Kubernetes deployment manifest for your application unchanged in all places where Tungsten Fabric with Kubernetes is available - on premises and in all major public clouds.","title":"When would I care?"},{"location":"use_case_1/#deployments","text":"When you create a Deployment, a CNI works in concert with Kubernetes to allocate network IP addresses for each of your application Pods, and \"wire up\" each pod to the cluster network. Note: most CNIs work by creating an overlay network that in most cases is contained within the boundaries of a single Kubernetes cluster. As the result, Pods in different clusters cannot communicate directly. While we will not cover multi-cluster scenarios in this document, Tungsten Fabric is capable of supporting such configurations. A single installation of Tungsten Fabric can serve multiple Kubernetes clusters at the same time. In such scenario Pods from different clusters can communicate with each other directly, even if Kubernetes clusters themselves are in different locations.","title":"Deployments"},{"location":"use_case_1/#services","text":"Services in Kubernetes are \"an abstract way to expose an application running on a set of Pods\". In most cases, a Service is a simple Round-Robin Load Balancer. It has a Virtual IP address (\"VIP\") for receiving network requests, and a zero or more Endpoint IP addresses that it will forward these requests to. In most cases a Service will automatically discover Endpoint IP addresses that belong to the application Pods by looking for matching labels (called \"Selectors\") on running Pods.","title":"Services"},{"location":"use_case_1/#sample-apps-deployments-and-services","text":"Make sure you're on the sandbox control node, logged in as root, and in the correct directory: # Make sure we're root whoami | grep root || sudo -s # Change to the manifests directory cd /home/centos/yelb/deployments/platformdeployment/Kubernetes/yaml Review the cnawebapp-loadbalancer.yaml file, looking for the sections that start with Kind: Deployment and Kind: Service less cnawebapp-loadbalancer.yaml (Use arrows / PgUp / PgDn to navigate; press q to exit) Note that: spec.template.spec.containers.ports.containerPort in Deployments shows what TCP port a Pod will listen to; spec.ports in Services shows what port a Service's VIP will be listening to; spec.selector in Services shows what labels on Pods a Service will look for to send traffic to. Next, let's deploy our app and see what happens: kubectl create -f cnawebapp-loadbalancer.yaml This will create the following application topology: If the application deploys without errors, we should be able to see that: All Pods have their own IP address and are listening on their respective ports: kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE redis-server-5786bc9958-mkm6s 1/1 Running 0 9m 10.47.255.247 ip-172-25-1-190.us-west-1.compute.internal yelb-appserver-7975bf6fb-xvzgh 1/1 Running 0 9m 10.47.255.248 ip-172-25-1-190.us-west-1.compute.internal yelb-db-ccd4bd9f5-9pt9m 1/1 Running 0 9m 10.47.255.249 ip-172-25-1-79.us-west-1.compute.internal yelb-ui-5cd947475f-dsvbd 1/1 Running 0 9m 10.47.255.250 ip-172-25-1-79.us-west-1.compute.internal All Services have a VIP and a port they're listening on: kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 4h redis-server ClusterIP 10.103.111.102 <none> 6379/TCP 9m yelb-appserver ClusterIP 10.109.212.183 <none> 4567/TCP 9m yelb-db ClusterIP 10.96.201.173 <none> 5432/TCP 9m yelb-ui LoadBalancer 10.108.170.55 aa01af9988cc3... 80:32393/TCP 9m All Services have discovered their respective Endpoints: kubectl get ep NAME ENDPOINTS AGE kubernetes 172.25.1.105:6443 4h redis-server 10.47.255.247:6379 10m yelb-appserver 10.47.255.248:4567 10m yelb-db 10.47.255.249:5432 10m yelb-ui 10.47.255.250:80 10m Since Tungsten Fabric provides support for Kubernetes Service type LoadBalancer , we should now be able to connect to our application from the Internet. Let's find out what the public DNS name for our load balancer is: # kubectl get svc yelb-ui -o wide NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR yelb-ui LoadBalancer 10.108.170.55 aa01af9988cc311e9badf06b57ebf630-1452353610.us-west-1.elb.amazonaws.com 80:32393/TCP 11m app=yelb-ui,tier=frontend We can see that our app is available on aa01af9988cc311e9badf06b57ebf630-1452353610.us-west-1.elb.amazonaws.com , so let's check it by pointing our web browser to it: It worked!","title":"Sample app's Deployments and Services"},{"location":"use_case_1/#cleanup","text":"Once you've played with the app for a moment or two, feel free to undeploy it: kubectl delete -f cnawebapp-loadbalancer.yaml","title":"Cleanup"},{"location":"use_case_1/#recap-and-whats-next","text":"In this Use Case, we've got our sample application deployed and available from the Internet using Tungsten Fabric Kubernetes CNI plugin and integration with AWS' Elastic Load Balancing. Our sample application is on the web. If this is all we wanted - we are done. However, if we need things like SSL offload or want to send incoming requests to different application components based on HTTP host and/or path, we will need to use Kubernetes Ingress. The Use Case 2 covers this scenario.","title":"Recap and what's next"},{"location":"use_case_2/","text":"Use case 2: Advanced external app connectivity through Kubernetes' Ingress Kubernetes documentation page on Ingress describes it as: \"An API object that manages external access to the services in a cluster, typically HTTP. Ingress can provide load balancing, SSL termination and name-based virtual hosting.\" Ingress functionality is not provided by CNI. This means that a Kubernetes cluster operator would typically have to install, and then manage and support a separate Ingress controller solution for their cluster. For Kubernetes deployments on premises and in public clouds where Kubernetes doesn't have built-in Ingress support, Tungsten Fabric comes bundled with its own Ingress controller. It uses HAProxy under the covers and implements all basic functionality as described in Kubernetes Ingress documentation page . When running on AWS, Kubernetes can be configured to use AWS's Application Load Balancer (ALB) for its Ingress services . Kubernetes in this sandbox is set up this way, to most closely reflect a typical real life deployment scenario. The following diagram outlines the resulting deployment architecture for our example application: When would I care? Ingress controller option is only compatible with applications that use HTTP or HTTPS. If that's the case for your application, then you may want to consider Ingress to help you with the following: You'd like to secure your application with HTTPS by exposing it through Ingress configured to perform SSL offload; and/or You'd like to direct incoming requests to different Kubernetes Services based on HTTP path in requests, e.g., /blog/ could go to Service A , while /account/ could go to Service B , etc.; and/or Your application serves multiple DNS domains through Name-based Virtual Hosting, e.g., requests with the Host: header set to test.project.com need to go to Service C , while ones with prod.project.com need to go to Service D . Exposing our sample app through Ingress Before we explore the three scenarios above, let's deploy our sample application with a simple Ingress similar to how we did it with Service type LoadBalancer , and then build on that. Make sure you're on the sandbox control node, logged in as root, and in the correct directory: # Make sure we're root whoami | grep root || sudo -s # Change to the manifests directory cd /home/centos/yelb/deployments/platformdeployment/Kubernetes/yaml # Deploy our sample application with Ingress kubectl create -f cnawebapp-ingress-alb.yaml After a couple of minutes, deployment process should complete and we should be able to access our sample application from the Internet. Let's find the DNS name for our Ingress: # kubectl get ing yelb-ui NAME HOSTS ADDRESS PORTS AGE yelb-ui * 539db10e-default-yelbui-3c9c-1330819777.us-west-1.elb.amazonaws.com 80 60m According to the output above, our sample app is now available from the Internet on http://539db10e-default-yelbui-3c9c-1330819777.us-west-1.elb.amazonaws.com Try accessing Yelb through the DNS name you've got when you ran the command above in your environment, to make sure it works. Securing our sample app with HTTPS For this exercise, we'll need to generate and add a self-signed certificate to the AWS Certificate Manager. AWS Application Load Balancer (ALB) that serves the Ingress function needs this to perform the encryption. Note: for production use, you will likely need to get a \"proper\" certificate for a fully registered domain name, probably through the respective function of the AWS Certificate Manager. Since we're just doing an exercise, we'll go with a self-signed one and a made-up domain. Step 1: Generate a self-signed certificate and add it to AWS Certificate Manager The steps below should be executed on a host where you have installed AWS CLI tool with Access and Secret keys that allow you to make changes to Certificate Manager. # Generate self-signed certificate for a fake domain \u2018yelb.mydomain.com': openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \"/CN=yelb.mydomain.com\" # Add our new certificate to the AWS Certificate Manager # Pay attention to the \"--region\" - it must be the same AWS region # where you're running Tungsten sandbox; in our example it is \"us-west-1\" aws acm import-certificate --certificate file://tls.crt --private-key file://tls.key --region us-west-1 If all went well, the last command will display something similar to the following: { \"CertificateArn\": \"arn:aws:acm:us-west-1:180612498884:certificate/e7341ff5-52ef-4a7b-94b5-05643ef6ab46\" } We will need the value that follows the CertificateArn for our next step. Step 2: Create our Ingress definition Make sure you're on the sandbox control node, logged in as root, and in the correct directory: # Make sure we're root whoami | grep root || sudo -s # Change to the manifests directory cd /home/centos/yelb/deployments/platformdeployment/Kubernetes/yaml Now, let's create a new Ingress definition: cat > ingress-https.yaml <<EOF apiVersion: extensions/v1beta1 kind: Ingress metadata: name: \"yelb-ui-https\" annotations: kubernetes.io/ingress.class: alb alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/listen-ports: '[{\"HTTPS\":443}]' alb.ingress.kubernetes.io/certificate-arn: INSERT_CERT_ARN_HERE labels: app: \"yelb-ui\" spec: rules: - host: yelb.mydomain.com http: paths: - path: /* backend: serviceName: \"yelb-ui\" servicePort: 80 EOF Next, let's put the CertificateArn in. Edit the command before running it, and replace the arn:aws:acm:us-west-1:180612498884:certificate/e7341ff5-52ef-4a7b-94b5-05643ef6ab46 with the value for CertificateArn that you got when you executed the Step 1 above. sed -i \"s#INSERT_CERT_ARN_HERE#arn:aws:acm:us-west-1:180612498884:certificate/e7341ff5-52ef-4a7b-94b5-05643ef6ab46#\" ingress-https.yaml If the command ran successfully, your ingress-https.yaml file will have the ARN of your self-signed certificate instead of the string INSERT_CERT_ARN_HERE . Step 3: Create the HTTPS Ingress and test it # Create a new Ingress kubectl create -f ingress-https.yaml After running the command above, wait for a couple minutes for the new ALB Ingress to come up. Then, let's find what DNS name has been assigned to it, and try connecting to it: # kubectl get ingress yelb-ui-https NAME HOSTS ADDRESS PORTS AGE yelb-ui-https yelb.mydomain.com 539db10e-default-yelbuihtt-2a0d-1983111448.us-west-1.elb.amazonaws.com 80 16m From the output above, we can see the ADDRESS of the new Ingress; let's see if it worked: # curl -v -k https://539db10e-default-yelbuihtt-2a0d-1983111448.us-west-1.elb.amazonaws.com -H \"Host: yelb.mydomain.com\" [..skip..] * SSL connection using TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 * Server certificate: * subject: CN=yelb.mydomain.com [..skip..] <html> <head> <script src=\"env.js\"></script> <meta charset=\"utf-8\"> <title>Yelb</title> <base href=\"/\"> <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"> <link rel=\"icon\" type=\"image/x-icon\" href=\"favicon.ico?v=2\"> </head> <body> <yelb>Loading...</yelb> [..skip..] It worked - we can access the Yelb application over an encrypted connection! Our new topology looks something like this (note that we still have our original HTTP Ingress that isn't shown on this diagram): Summary: why would I want this? There are a few benefits to implementing HTTPS Ingress, in addition to added end-user's connection security, privacy, and data integrity: Your application consumes less compute resources, since the encryption overheads are transferred to the ALB; Your application now supports HTTP/2, which is a good thing ; You can also easily implement automatic redirect of HTTP to HTTPS. Cleanup Let's delete the HTTPS Ingress that we've added as we don't need it for the rest of this chapter: kubectl delete -f ingress-https.yaml Then, from the computer where you performed the Step 1 (generate and install self-signed certificate into the AWS Certificate Manager), run the following command to delete that certificate, making sure you use your own value of the CertificateArn : aws acm delete-certificate --certificate-arn arn:aws:acm:us-west-1:180612498884:certificate/e7341ff5-52ef-4a7b-94b5-05643ef6ab46 Directing requests based on URL path In some cases you may want to run more than one application off the same DNS domain name; for example www.corp.com may support your main application, while some other application like WordPress could be handling www.corp.com/blog . For this exercise, we are assuming that you have a copy of Yelb running per instructions at the start of this chapter in \"Exposing our sample app through Ingress\". If you're starting fresh, jump up to that part, follow the instructions to deploy, and come back. To demonstrate the routing by the URL Path, we will add another Deployment to our environment, and update the configuration of our Ingress accordingly. With this new configuration our Ingress will direct requests for the / path to our main app yelb , while requests for the /echo path will go to the new app, EchoServer . Here's a diagram of our target state: We should already have the Yelb part in place, so let's add the EchoServer : # Create the EchoServer Deployment and Service manifest: cat > echoserver.yaml << EOF apiVersion: extensions/v1beta1 kind: Deployment metadata: name: echoserver spec: replicas: 1 template: metadata: labels: app: echoserver spec: containers: - image: gcr.io/google_containers/echoserver:1.4 name: echoserver ports: - containerPort: 8080 --- apiVersion: v1 kind: Service metadata: name: echoserver spec: ports: - port: 80 targetPort: 8080 protocol: TCP type: NodePort selector: app: echoserver EOF # And now deploy it: kubectl create -f echoserver.yaml Next, we'll create an updated configuration for our Ingress. To do this, we'll copy the Ingress resource from the cnawebapp-ingress-alb.yaml and make two changes in the routing part: Update the Path to yelb from /* to / so that it doesn't interfere with the echoserver ; and Add a new /echo Path leading to the echoserver Note: the reason we're including the full resource definition instead of just applying the difference is that Ingress objects don't support strategic merge patching. # Our updated Ingress resource: cat > ingress-paths.yaml << EOF apiVersion: extensions/v1beta1 kind: Ingress metadata: name: \"yelb-ui\" annotations: kubernetes.io/ingress.class: alb alb.ingress.kubernetes.io/scheme: internet-facing labels: app: \"yelb-ui\" spec: rules: - http: paths: - path: / backend: serviceName: \"yelb-ui\" servicePort: 80 - path: /echo backend: serviceName: \"echoserver\" servicePort: 80 EOF # And now deploy it: kubectl apply -f ingress-paths.yaml There will be a warning about the kubectl apply which is safe to ignore in our case since our updated resource is essentially the same save the rules configuration. Once the updated configuration becomes active in a few seconds, we can check if the URL-based routing works. When we request the base URL / (or empty), we should get to Yelb , and if we request /echo , we should get back output of the EchoServer . # Get the base URL of our Ingress baseUrl=$(kubectl get ing yelb-ui | grep amaz | awk '{print $3}') echo \"Our Ingress is at: ${baseUrl}\" # Try to access the $baseUrl ; we should get back our Yelb UI page contents curl http://${baseUrl} # Now try /echo ; we should get back the output of the EchoServer curl http://${baseUrl}/echo Serving multiple DNS domains The solution in this scenario is useful when you have multiple domain names and would like to serve different applications from each domain while sharing the same Ingress infrastructure. This helps save costs and in some cases may be less complex than having dedicated Ingress instances per domain name. This exercise builds on the previous one, Directing requests based on URL path . If you have not completed it, please go up and simply cut and paste the steps that create and deploy the echoserver.yaml manifest. We will build a new one for the Ingress, so no need to create and deploy the ingress-paths.yaml . Once ready, you should have: a copy of yelb and a copy of echoserver . It doesn't matter what your Ingress configuration is as we're going to overwrite it. In our target state, Ingress will have two domain names defined, yelb.mydomain.com and echo.mydomain.com , and will route the incoming requests according to the value in the Host: HTTP header, which web browsers insert automatically for the host portion of the URL that you request. Here's the diagram of our target state: Let's create and deploy the configuration for our Ingress that will do the required routing: # Our updated Ingress resource: cat > ingress-hosts.yaml << EOF apiVersion: extensions/v1beta1 kind: Ingress metadata: name: \"yelb-ui\" annotations: kubernetes.io/ingress.class: alb alb.ingress.kubernetes.io/scheme: internet-facing labels: app: \"yelb-ui\" spec: rules: - host: yelb.mydomain.com http: paths: - path: /* backend: serviceName: \"yelb-ui\" servicePort: 80 - host: echo.mydomain.com http: paths: - path: /* backend: serviceName: \"echoserver\" servicePort: 80 EOF # And now deploy it: kubectl apply -f ingress-hosts.yaml Once the configuration successfully applies, we're ready to test. Since the domain name and hosts are made up, we will ask curl to add a correct Host: header. When it is set to yelb.mydomain.com we should get to Yelb , and when it's set to echo.mydomain.com , we should get back output of the EchoServer . # Get the base URL of our Ingress baseUrl=$(kubectl get ing yelb-ui | grep amaz | awk '{print $3}') echo \"Our Ingress is at: ${baseUrl}\" # Access our Ingress with Host: set to yelb; we should get back our Yelb UI page contents curl http://${baseUrl} -H \"Host: yelb.mydomain.com\" # Now try Host: set to echo; we should get back the output of the EchoServer curl http://${baseUrl} -H \"Host: echo.mydomain.com\" Cleanup Once you've explored enough, feel free to clean things up: # Delete the \"yelb\" and \"echoserver\" applications: kubectl delete -f cnawebapp-ingress-alb.yaml kubectl delete -f echoserver.yaml # Delete the additional manifests we've created: rm -f echoserver.yaml ingress-paths.yaml ingress-hosts.yaml Recap and what's next Kubernetes provides three basic ways of exposing an application to the outside world: Services of type LoadBalancer or NodePort , and Ingress. The first two support any arbitrary protocols, but also don't add much in the sense of protocol intelligence. Ingress, on the other hand, provides protocol-specific functions, but that makes it compatible only with applications that use HTTP or HTTPS. Similar to other features, Kubernetes requires a controller to implement the actual Ingress functionality - simply creating Ingress resources in Kubernetes API doesn't do anything. Ingress controllers are pieces of software that Kubernetes cluster operators have to install and then monitor, patch, upgrade, etc.. Tungsten Fabric comes bundled with an Ingress controller which should make this process easier. Once you've determined how your application should be exposed to the Internet, you need to consider how to deal with the questions around network access controls. Read on Use Cases 3 and 4 in this guide where we cover some of these scenarios.","title":"Use case 2"},{"location":"use_case_2/#use-case-2-advanced-external-app-connectivity-through-kubernetes-ingress","text":"Kubernetes documentation page on Ingress describes it as: \"An API object that manages external access to the services in a cluster, typically HTTP. Ingress can provide load balancing, SSL termination and name-based virtual hosting.\" Ingress functionality is not provided by CNI. This means that a Kubernetes cluster operator would typically have to install, and then manage and support a separate Ingress controller solution for their cluster. For Kubernetes deployments on premises and in public clouds where Kubernetes doesn't have built-in Ingress support, Tungsten Fabric comes bundled with its own Ingress controller. It uses HAProxy under the covers and implements all basic functionality as described in Kubernetes Ingress documentation page . When running on AWS, Kubernetes can be configured to use AWS's Application Load Balancer (ALB) for its Ingress services . Kubernetes in this sandbox is set up this way, to most closely reflect a typical real life deployment scenario. The following diagram outlines the resulting deployment architecture for our example application:","title":"Use case 2: Advanced external app connectivity through Kubernetes' Ingress"},{"location":"use_case_2/#when-would-i-care","text":"Ingress controller option is only compatible with applications that use HTTP or HTTPS. If that's the case for your application, then you may want to consider Ingress to help you with the following: You'd like to secure your application with HTTPS by exposing it through Ingress configured to perform SSL offload; and/or You'd like to direct incoming requests to different Kubernetes Services based on HTTP path in requests, e.g., /blog/ could go to Service A , while /account/ could go to Service B , etc.; and/or Your application serves multiple DNS domains through Name-based Virtual Hosting, e.g., requests with the Host: header set to test.project.com need to go to Service C , while ones with prod.project.com need to go to Service D .","title":"When would I care?"},{"location":"use_case_2/#exposing-our-sample-app-through-ingress","text":"Before we explore the three scenarios above, let's deploy our sample application with a simple Ingress similar to how we did it with Service type LoadBalancer , and then build on that. Make sure you're on the sandbox control node, logged in as root, and in the correct directory: # Make sure we're root whoami | grep root || sudo -s # Change to the manifests directory cd /home/centos/yelb/deployments/platformdeployment/Kubernetes/yaml # Deploy our sample application with Ingress kubectl create -f cnawebapp-ingress-alb.yaml After a couple of minutes, deployment process should complete and we should be able to access our sample application from the Internet. Let's find the DNS name for our Ingress: # kubectl get ing yelb-ui NAME HOSTS ADDRESS PORTS AGE yelb-ui * 539db10e-default-yelbui-3c9c-1330819777.us-west-1.elb.amazonaws.com 80 60m According to the output above, our sample app is now available from the Internet on http://539db10e-default-yelbui-3c9c-1330819777.us-west-1.elb.amazonaws.com Try accessing Yelb through the DNS name you've got when you ran the command above in your environment, to make sure it works.","title":"Exposing our sample app through Ingress"},{"location":"use_case_2/#securing-our-sample-app-with-https","text":"For this exercise, we'll need to generate and add a self-signed certificate to the AWS Certificate Manager. AWS Application Load Balancer (ALB) that serves the Ingress function needs this to perform the encryption. Note: for production use, you will likely need to get a \"proper\" certificate for a fully registered domain name, probably through the respective function of the AWS Certificate Manager. Since we're just doing an exercise, we'll go with a self-signed one and a made-up domain.","title":"Securing our sample app with HTTPS"},{"location":"use_case_2/#step-1-generate-a-self-signed-certificate-and-add-it-to-aws-certificate-manager","text":"The steps below should be executed on a host where you have installed AWS CLI tool with Access and Secret keys that allow you to make changes to Certificate Manager. # Generate self-signed certificate for a fake domain \u2018yelb.mydomain.com': openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \"/CN=yelb.mydomain.com\" # Add our new certificate to the AWS Certificate Manager # Pay attention to the \"--region\" - it must be the same AWS region # where you're running Tungsten sandbox; in our example it is \"us-west-1\" aws acm import-certificate --certificate file://tls.crt --private-key file://tls.key --region us-west-1 If all went well, the last command will display something similar to the following: { \"CertificateArn\": \"arn:aws:acm:us-west-1:180612498884:certificate/e7341ff5-52ef-4a7b-94b5-05643ef6ab46\" } We will need the value that follows the CertificateArn for our next step.","title":"Step 1: Generate a self-signed certificate and add it to AWS Certificate Manager"},{"location":"use_case_2/#step-2-create-our-ingress-definition","text":"Make sure you're on the sandbox control node, logged in as root, and in the correct directory: # Make sure we're root whoami | grep root || sudo -s # Change to the manifests directory cd /home/centos/yelb/deployments/platformdeployment/Kubernetes/yaml Now, let's create a new Ingress definition: cat > ingress-https.yaml <<EOF apiVersion: extensions/v1beta1 kind: Ingress metadata: name: \"yelb-ui-https\" annotations: kubernetes.io/ingress.class: alb alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/listen-ports: '[{\"HTTPS\":443}]' alb.ingress.kubernetes.io/certificate-arn: INSERT_CERT_ARN_HERE labels: app: \"yelb-ui\" spec: rules: - host: yelb.mydomain.com http: paths: - path: /* backend: serviceName: \"yelb-ui\" servicePort: 80 EOF Next, let's put the CertificateArn in. Edit the command before running it, and replace the arn:aws:acm:us-west-1:180612498884:certificate/e7341ff5-52ef-4a7b-94b5-05643ef6ab46 with the value for CertificateArn that you got when you executed the Step 1 above. sed -i \"s#INSERT_CERT_ARN_HERE#arn:aws:acm:us-west-1:180612498884:certificate/e7341ff5-52ef-4a7b-94b5-05643ef6ab46#\" ingress-https.yaml If the command ran successfully, your ingress-https.yaml file will have the ARN of your self-signed certificate instead of the string INSERT_CERT_ARN_HERE .","title":"Step 2: Create our Ingress definition"},{"location":"use_case_2/#step-3-create-the-https-ingress-and-test-it","text":"# Create a new Ingress kubectl create -f ingress-https.yaml After running the command above, wait for a couple minutes for the new ALB Ingress to come up. Then, let's find what DNS name has been assigned to it, and try connecting to it: # kubectl get ingress yelb-ui-https NAME HOSTS ADDRESS PORTS AGE yelb-ui-https yelb.mydomain.com 539db10e-default-yelbuihtt-2a0d-1983111448.us-west-1.elb.amazonaws.com 80 16m From the output above, we can see the ADDRESS of the new Ingress; let's see if it worked: # curl -v -k https://539db10e-default-yelbuihtt-2a0d-1983111448.us-west-1.elb.amazonaws.com -H \"Host: yelb.mydomain.com\" [..skip..] * SSL connection using TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 * Server certificate: * subject: CN=yelb.mydomain.com [..skip..] <html> <head> <script src=\"env.js\"></script> <meta charset=\"utf-8\"> <title>Yelb</title> <base href=\"/\"> <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"> <link rel=\"icon\" type=\"image/x-icon\" href=\"favicon.ico?v=2\"> </head> <body> <yelb>Loading...</yelb> [..skip..] It worked - we can access the Yelb application over an encrypted connection! Our new topology looks something like this (note that we still have our original HTTP Ingress that isn't shown on this diagram):","title":"Step 3: Create the HTTPS Ingress and test it"},{"location":"use_case_2/#summary-why-would-i-want-this","text":"There are a few benefits to implementing HTTPS Ingress, in addition to added end-user's connection security, privacy, and data integrity: Your application consumes less compute resources, since the encryption overheads are transferred to the ALB; Your application now supports HTTP/2, which is a good thing ; You can also easily implement automatic redirect of HTTP to HTTPS.","title":"Summary: why would I want this?"},{"location":"use_case_2/#cleanup","text":"Let's delete the HTTPS Ingress that we've added as we don't need it for the rest of this chapter: kubectl delete -f ingress-https.yaml Then, from the computer where you performed the Step 1 (generate and install self-signed certificate into the AWS Certificate Manager), run the following command to delete that certificate, making sure you use your own value of the CertificateArn : aws acm delete-certificate --certificate-arn arn:aws:acm:us-west-1:180612498884:certificate/e7341ff5-52ef-4a7b-94b5-05643ef6ab46","title":"Cleanup"},{"location":"use_case_2/#directing-requests-based-on-url-path","text":"In some cases you may want to run more than one application off the same DNS domain name; for example www.corp.com may support your main application, while some other application like WordPress could be handling www.corp.com/blog . For this exercise, we are assuming that you have a copy of Yelb running per instructions at the start of this chapter in \"Exposing our sample app through Ingress\". If you're starting fresh, jump up to that part, follow the instructions to deploy, and come back. To demonstrate the routing by the URL Path, we will add another Deployment to our environment, and update the configuration of our Ingress accordingly. With this new configuration our Ingress will direct requests for the / path to our main app yelb , while requests for the /echo path will go to the new app, EchoServer . Here's a diagram of our target state: We should already have the Yelb part in place, so let's add the EchoServer : # Create the EchoServer Deployment and Service manifest: cat > echoserver.yaml << EOF apiVersion: extensions/v1beta1 kind: Deployment metadata: name: echoserver spec: replicas: 1 template: metadata: labels: app: echoserver spec: containers: - image: gcr.io/google_containers/echoserver:1.4 name: echoserver ports: - containerPort: 8080 --- apiVersion: v1 kind: Service metadata: name: echoserver spec: ports: - port: 80 targetPort: 8080 protocol: TCP type: NodePort selector: app: echoserver EOF # And now deploy it: kubectl create -f echoserver.yaml Next, we'll create an updated configuration for our Ingress. To do this, we'll copy the Ingress resource from the cnawebapp-ingress-alb.yaml and make two changes in the routing part: Update the Path to yelb from /* to / so that it doesn't interfere with the echoserver ; and Add a new /echo Path leading to the echoserver Note: the reason we're including the full resource definition instead of just applying the difference is that Ingress objects don't support strategic merge patching. # Our updated Ingress resource: cat > ingress-paths.yaml << EOF apiVersion: extensions/v1beta1 kind: Ingress metadata: name: \"yelb-ui\" annotations: kubernetes.io/ingress.class: alb alb.ingress.kubernetes.io/scheme: internet-facing labels: app: \"yelb-ui\" spec: rules: - http: paths: - path: / backend: serviceName: \"yelb-ui\" servicePort: 80 - path: /echo backend: serviceName: \"echoserver\" servicePort: 80 EOF # And now deploy it: kubectl apply -f ingress-paths.yaml There will be a warning about the kubectl apply which is safe to ignore in our case since our updated resource is essentially the same save the rules configuration. Once the updated configuration becomes active in a few seconds, we can check if the URL-based routing works. When we request the base URL / (or empty), we should get to Yelb , and if we request /echo , we should get back output of the EchoServer . # Get the base URL of our Ingress baseUrl=$(kubectl get ing yelb-ui | grep amaz | awk '{print $3}') echo \"Our Ingress is at: ${baseUrl}\" # Try to access the $baseUrl ; we should get back our Yelb UI page contents curl http://${baseUrl} # Now try /echo ; we should get back the output of the EchoServer curl http://${baseUrl}/echo","title":"Directing requests based on URL path"},{"location":"use_case_2/#serving-multiple-dns-domains","text":"The solution in this scenario is useful when you have multiple domain names and would like to serve different applications from each domain while sharing the same Ingress infrastructure. This helps save costs and in some cases may be less complex than having dedicated Ingress instances per domain name. This exercise builds on the previous one, Directing requests based on URL path . If you have not completed it, please go up and simply cut and paste the steps that create and deploy the echoserver.yaml manifest. We will build a new one for the Ingress, so no need to create and deploy the ingress-paths.yaml . Once ready, you should have: a copy of yelb and a copy of echoserver . It doesn't matter what your Ingress configuration is as we're going to overwrite it. In our target state, Ingress will have two domain names defined, yelb.mydomain.com and echo.mydomain.com , and will route the incoming requests according to the value in the Host: HTTP header, which web browsers insert automatically for the host portion of the URL that you request. Here's the diagram of our target state: Let's create and deploy the configuration for our Ingress that will do the required routing: # Our updated Ingress resource: cat > ingress-hosts.yaml << EOF apiVersion: extensions/v1beta1 kind: Ingress metadata: name: \"yelb-ui\" annotations: kubernetes.io/ingress.class: alb alb.ingress.kubernetes.io/scheme: internet-facing labels: app: \"yelb-ui\" spec: rules: - host: yelb.mydomain.com http: paths: - path: /* backend: serviceName: \"yelb-ui\" servicePort: 80 - host: echo.mydomain.com http: paths: - path: /* backend: serviceName: \"echoserver\" servicePort: 80 EOF # And now deploy it: kubectl apply -f ingress-hosts.yaml Once the configuration successfully applies, we're ready to test. Since the domain name and hosts are made up, we will ask curl to add a correct Host: header. When it is set to yelb.mydomain.com we should get to Yelb , and when it's set to echo.mydomain.com , we should get back output of the EchoServer . # Get the base URL of our Ingress baseUrl=$(kubectl get ing yelb-ui | grep amaz | awk '{print $3}') echo \"Our Ingress is at: ${baseUrl}\" # Access our Ingress with Host: set to yelb; we should get back our Yelb UI page contents curl http://${baseUrl} -H \"Host: yelb.mydomain.com\" # Now try Host: set to echo; we should get back the output of the EchoServer curl http://${baseUrl} -H \"Host: echo.mydomain.com\"","title":"Serving multiple DNS domains"},{"location":"use_case_2/#cleanup_1","text":"Once you've explored enough, feel free to clean things up: # Delete the \"yelb\" and \"echoserver\" applications: kubectl delete -f cnawebapp-ingress-alb.yaml kubectl delete -f echoserver.yaml # Delete the additional manifests we've created: rm -f echoserver.yaml ingress-paths.yaml ingress-hosts.yaml","title":"Cleanup"},{"location":"use_case_2/#recap-and-whats-next","text":"Kubernetes provides three basic ways of exposing an application to the outside world: Services of type LoadBalancer or NodePort , and Ingress. The first two support any arbitrary protocols, but also don't add much in the sense of protocol intelligence. Ingress, on the other hand, provides protocol-specific functions, but that makes it compatible only with applications that use HTTP or HTTPS. Similar to other features, Kubernetes requires a controller to implement the actual Ingress functionality - simply creating Ingress resources in Kubernetes API doesn't do anything. Ingress controllers are pieces of software that Kubernetes cluster operators have to install and then monitor, patch, upgrade, etc.. Tungsten Fabric comes bundled with an Ingress controller which should make this process easier. Once you've determined how your application should be exposed to the Internet, you need to consider how to deal with the questions around network access controls. Read on Use Cases 3 and 4 in this guide where we cover some of these scenarios.","title":"Recap and what's next"},{"location":"use_case_3/","text":"Use case 3: Coarse application isolation through Kubernetes Namespaces Kubernetes Namespaces is a built-in means to \"virtualise\" Kubernetes clusters. While the jury is currently out regarding how and where to use Namespaces, cluster virtualisation can not be complete without the ability to isolate namespaces network-wise. Tungsten Fabric Kubernetes CNI plugin includes support for isolated Namespaces. An application deployed into an isolated Namespace cannot reach any Pods outside the Namespace it's in, and its Pods and Services cannot be reached by applications from other Namespaces. When would I care? One approach that organisations use is to deploy separate Kubernetes clusters per development team, in which case there's little benefit to having cluster virtualisation and namespace separation. However this approach may lead to inefficient use of resources, because unused capacity is fragmented. Each cluster has its own free capacity that cannot be used by applications running in other clusters. Additionally, as the number of clusters grows, it introduces operational overhead to keep things uniform. And last but not least, it takes time to bring up a new cluster, which may slow things down. Namespaces are a good way to work around these problems as they help to reduce the number of clusters, share the spare capacity, and are quick to create. They can also provide a level of separation where an infrastructure team would look after the clusters, while individual developer teams operate within their own namespaces. There are three general areas that need to be addressed when dealing with cluster virtualisation: (1) who can access the virtual cluster (RBAC); (2) how much compute resources each virtual cluster can use; and (3) what network communications are allowed for applications in virtual clusters. Tungsten Fabric CNI plugin for Kubernetes is designed to help with the (3) through Namespace isolation that this section will talk about, and NetworkPolicy that is covered by the next section. This is especially useful from a regulatory compliance standpoint. PCI compliance is a great example, as it encourages workload isolation. When looking to achieve PCI compliance, one of the key areas of focus is scope reduction. The goal of scope reduction is to isolate all systems which can affect the systems which process credit card information, known as the Cardholder Data Environment (CDE) in any way. Any workload or device that can interact in any way with a system that is part of the CDE is considered in scope. Network segmentation is critical to achieving the isolation required to reduce the number of systems that would be considered in scope for PCI compliance purposes. Kubernetes namespaces, and the underlying containerization platforms Kubernetes orchestrates, provide the compute isolation needed to reduce PCI scope for containerized workloads. Kubernetes also provides part of the solution regarding storage isolation, however, Kubernetes currently does not offer adequate network isolation or inspection for this purpose. Tungsten Fabric CNI plugin for Kubernetes not only provides Kubernetes namespace-aware network isolation capabilities, it also provides operations teams the ability to inspect all network traffic in and out of a namespace by steering that traffic through Network Functions Virtualization (NFV) instances. This allows for the level of data flow inspection to be adjusted as required by the types of communication that must be allowed in and out of the isolated CDE. Let's explore an example of network isolation using Kubernetes namespaces. In this use case, we will deploy two copies of our sample app, one into the default Namespace, and one into a new isolated Namespace. We will then see how Tungsten Fabric enforces network communication isolation, as shown on this diagram: Adding an isolated Namespace Before jumping in, it's worth to quickly scan Kubernetes documentation page that explains how to work with Namespaces, including commands we will need to know. All done? Make sure you're on the sandbox control node, logged in as root, and in the correct directory: # Make sure we're root whoami | grep root || sudo -s # Change to the manifests directory cd /home/centos/yelb/deployments/platformdeployment/Kubernetes/yaml Next, let's create a new manifest that describes our new isolated Namespace: cat > dev-isolated.yaml <<EOF apiVersion: v1 kind: Namespace metadata: name: \"dev-isolated\" annotations: { \"opencontrail.org/isolation\" : \"true\" } EOF This should create a file called dev-isolated.yaml with the contents above. Note the annotations section - this is what will tell Tungsten Fabric to make your new Namespace isolated. Go ahead to create this namespace and add a new context to Kubernetes config file so that we can access it: # Create our new Namespace: kubectl create -f dev-isolated.yaml Let's have a quick look at our new Namespace: # kubectl describe ns dev-isolated Name: dev-isolated Labels: <none> Annotations: opencontrail.org/isolation=true Status: Active No resource quota. No resource limits. Note the Annotations field; this signalled to Tungsten Fabric CNI plugin that it needs to treat this Namespace differently. Could we have simply added this annotation to an existing Namespace to make it isolated? Unfortunately no, because Tungsten has to do a fair bit of additional work to set up a new Namespace as isolated. More specifically, if has to create a set of separate Virtual Networks that applications Pods in this Namespace will be connected to. This ensures the network separation is maintained at a fundamental level, rather than simply through weaker methods like traffic filters. Deploy sample app into an isolated Namespace Next we will deploy our sample application into the isolated Namespace that we've created: kubectl create --namespace dev-isolated -f cnawebapp-loadbalancer.yaml Once the application pods are up, we should be able to access our application from the Internet same as described in Use case 1 above. Next we'll need something to compare and contrast with; so let's deploy another copy of our sample app, but this time into the default Namespace: kubectl create -f cnawebapp-loadbalancer.yaml Now we have two copies of our app; one is running in the default Namespace that is not isolated, the other is in dev-isolated Namespace that is. The behaviour that we expect: Pods and Services in non-isolated Namespace should be reachable from other Pods in non-isolated Namespaces (such as default and kube-system ) Services in non-isolated Namespaces should be reachable from Pods running in isolated Namespaces Pods and Services in isolated Namespaces should only be reachable from Pods in the same Namespace Exception to the above: Services of type LoadBalancer in isolated Namespaces will be reachable from the outside world. Let's validate this point by point. Pods in non-isolated Namespaces should be able to talk We know that Pods can talk to Services in our default namespace - that's how our sample app works. But what about across namespaces? Since we're in a sandbox, we could use one of Pods in kube-system namespace to try reach Pods and Services in our app that's running in the non-isolated namespace default : # Get the name of one of the kube-system pods; tiller-deploy: src_pod=$(kubectl get pods --namespace kube-system | grep tiller | awk '{print $1}') # Now let's get an IP of the \"yelb-ui\" pod in \"default\" namespace: dst_pod_ip=$(kubectl get pods -o wide | grep yelb-ui | awk '{print $6}') # Now let's ask tiller-deploy to ping yelb-ui: kubectl exec --namespace kube-system -it ${src_pod} ping ${dst_pod_ip} The last command should result in output similar to: PING 10.47.255.246 (10.47.255.246): 56 data bytes 64 bytes from 10.47.255.246: seq=0 ttl=63 time=1.291 ms 64 bytes from 10.47.255.246: seq=1 ttl=63 time=0.576 ms Cancel the command with ^C. This confirms that pods in non-isolated namespaces can reach each other. Isolated Pods should be able to reach non-isolated Services # Get the Cluster IP of the `yelb-ui` Service running in the `default` Namespace: default_yelb_ui_ip=$(kubectl get svc --namespace default -o wide | grep yelb-ui | awk '{print $3'}) # Get the name of \"yelb-appserver\" Pod in the \"dev-isolated\" Namespace: src_pod2=$(kubectl get pods --namespace dev-isolated | grep yelb-appserver | awk '{print $1}') # Run \"curl\" on \"yelb-appserver\" trying to reach the Service IP in \"default\" Namespace: kubectl exec -it -n dev-isolated ${src_pod2} -- /usr/bin/curl http://${default_yelb_ui_ip} We should see ~10 lines of HTML code of our main yelb-ui page, which shows that a Pod in dev-isolated Namespace can talk to a Service in non-isolated default Namespace. Pods in an isolated Namespace should not be reachable from other Namespaces Now, let's try to ping yelb-ui Pod that's running in the dev-isolated Namespace from the same tiller-deploy Pod: # Get an IP of the \"yelb-ui\" pod in \"dev-isolated\" namespace: isolated_pod_ip=$(kubectl get pods --namespace dev-isolated -o wide | grep yelb-ui | awk '{print $6}') # ..and try pinging it: kubectl exec --namespace kube-system -it ${src_pod} ping ${isolated_pod_ip} You should see that the command is \"stuck\", not displaying any responses because this time we're trying to reach something that isn't reachable because Tungsten Fabric is preventing it. Press ^C to cancel the command. Experiment a bit more on your own - try pinging isolated yelb Pods and Services from yelb Pods in default Namespace. Is everything working as you expected it to? LoadBalancer Services in isolated Namespaces should be accessible outside It wouldn't be much point to run application in an isolated Namespace if we couldn't access it, though. Therefore, our copy of yelb that is running in an isolated Namespace dev-isolated should be available to the Internet through the LoadBalancer Service yelb-ui . Let's test it: kubectl get svc --namespace dev-isolated -o wide | grep yelb-ui | awk '{print $4}' It should display something like afd9047c2915911e9b411026463a4a33-777914712.us-west-1.elb.amazonaws.com ; point your browser to it and see if our application loads! Cleanup Once you've explored enough, feel free to clean things up: # Delete the two copies of \"yelb\": kubectl delete -f cnawebapp-loadbalancer.yaml kubectl delete --namespace dev-isolated -f cnawebapp-loadbalancer.yaml # Delete the isolated namespace and its Manifest: kubectl delete -f dev-isolated.yaml rm -f dev-isolated.yaml Recap and what's next Kubernetes Namespaces have been designed as a means of virtualising Kubernetes clusters. No virtualisation is complete without networking, and Tungsten Fabric's support for isolated Namespaces provides this function. Isolated Namespaces however have low granularity that you may need when implementing application network security policies within your Namespace. To get familiar with these more fine controls, check out the Use Case 4.","title":"Use case 3"},{"location":"use_case_3/#use-case-3-coarse-application-isolation-through-kubernetes-namespaces","text":"Kubernetes Namespaces is a built-in means to \"virtualise\" Kubernetes clusters. While the jury is currently out regarding how and where to use Namespaces, cluster virtualisation can not be complete without the ability to isolate namespaces network-wise. Tungsten Fabric Kubernetes CNI plugin includes support for isolated Namespaces. An application deployed into an isolated Namespace cannot reach any Pods outside the Namespace it's in, and its Pods and Services cannot be reached by applications from other Namespaces.","title":"Use case 3: Coarse application isolation through Kubernetes Namespaces"},{"location":"use_case_3/#when-would-i-care","text":"One approach that organisations use is to deploy separate Kubernetes clusters per development team, in which case there's little benefit to having cluster virtualisation and namespace separation. However this approach may lead to inefficient use of resources, because unused capacity is fragmented. Each cluster has its own free capacity that cannot be used by applications running in other clusters. Additionally, as the number of clusters grows, it introduces operational overhead to keep things uniform. And last but not least, it takes time to bring up a new cluster, which may slow things down. Namespaces are a good way to work around these problems as they help to reduce the number of clusters, share the spare capacity, and are quick to create. They can also provide a level of separation where an infrastructure team would look after the clusters, while individual developer teams operate within their own namespaces. There are three general areas that need to be addressed when dealing with cluster virtualisation: (1) who can access the virtual cluster (RBAC); (2) how much compute resources each virtual cluster can use; and (3) what network communications are allowed for applications in virtual clusters. Tungsten Fabric CNI plugin for Kubernetes is designed to help with the (3) through Namespace isolation that this section will talk about, and NetworkPolicy that is covered by the next section. This is especially useful from a regulatory compliance standpoint. PCI compliance is a great example, as it encourages workload isolation. When looking to achieve PCI compliance, one of the key areas of focus is scope reduction. The goal of scope reduction is to isolate all systems which can affect the systems which process credit card information, known as the Cardholder Data Environment (CDE) in any way. Any workload or device that can interact in any way with a system that is part of the CDE is considered in scope. Network segmentation is critical to achieving the isolation required to reduce the number of systems that would be considered in scope for PCI compliance purposes. Kubernetes namespaces, and the underlying containerization platforms Kubernetes orchestrates, provide the compute isolation needed to reduce PCI scope for containerized workloads. Kubernetes also provides part of the solution regarding storage isolation, however, Kubernetes currently does not offer adequate network isolation or inspection for this purpose. Tungsten Fabric CNI plugin for Kubernetes not only provides Kubernetes namespace-aware network isolation capabilities, it also provides operations teams the ability to inspect all network traffic in and out of a namespace by steering that traffic through Network Functions Virtualization (NFV) instances. This allows for the level of data flow inspection to be adjusted as required by the types of communication that must be allowed in and out of the isolated CDE. Let's explore an example of network isolation using Kubernetes namespaces. In this use case, we will deploy two copies of our sample app, one into the default Namespace, and one into a new isolated Namespace. We will then see how Tungsten Fabric enforces network communication isolation, as shown on this diagram:","title":"When would I care?"},{"location":"use_case_3/#adding-an-isolated-namespace","text":"Before jumping in, it's worth to quickly scan Kubernetes documentation page that explains how to work with Namespaces, including commands we will need to know. All done? Make sure you're on the sandbox control node, logged in as root, and in the correct directory: # Make sure we're root whoami | grep root || sudo -s # Change to the manifests directory cd /home/centos/yelb/deployments/platformdeployment/Kubernetes/yaml Next, let's create a new manifest that describes our new isolated Namespace: cat > dev-isolated.yaml <<EOF apiVersion: v1 kind: Namespace metadata: name: \"dev-isolated\" annotations: { \"opencontrail.org/isolation\" : \"true\" } EOF This should create a file called dev-isolated.yaml with the contents above. Note the annotations section - this is what will tell Tungsten Fabric to make your new Namespace isolated. Go ahead to create this namespace and add a new context to Kubernetes config file so that we can access it: # Create our new Namespace: kubectl create -f dev-isolated.yaml Let's have a quick look at our new Namespace: # kubectl describe ns dev-isolated Name: dev-isolated Labels: <none> Annotations: opencontrail.org/isolation=true Status: Active No resource quota. No resource limits. Note the Annotations field; this signalled to Tungsten Fabric CNI plugin that it needs to treat this Namespace differently. Could we have simply added this annotation to an existing Namespace to make it isolated? Unfortunately no, because Tungsten has to do a fair bit of additional work to set up a new Namespace as isolated. More specifically, if has to create a set of separate Virtual Networks that applications Pods in this Namespace will be connected to. This ensures the network separation is maintained at a fundamental level, rather than simply through weaker methods like traffic filters.","title":"Adding an isolated Namespace"},{"location":"use_case_3/#deploy-sample-app-into-an-isolated-namespace","text":"Next we will deploy our sample application into the isolated Namespace that we've created: kubectl create --namespace dev-isolated -f cnawebapp-loadbalancer.yaml Once the application pods are up, we should be able to access our application from the Internet same as described in Use case 1 above. Next we'll need something to compare and contrast with; so let's deploy another copy of our sample app, but this time into the default Namespace: kubectl create -f cnawebapp-loadbalancer.yaml Now we have two copies of our app; one is running in the default Namespace that is not isolated, the other is in dev-isolated Namespace that is. The behaviour that we expect: Pods and Services in non-isolated Namespace should be reachable from other Pods in non-isolated Namespaces (such as default and kube-system ) Services in non-isolated Namespaces should be reachable from Pods running in isolated Namespaces Pods and Services in isolated Namespaces should only be reachable from Pods in the same Namespace Exception to the above: Services of type LoadBalancer in isolated Namespaces will be reachable from the outside world. Let's validate this point by point.","title":"Deploy sample app into an isolated Namespace"},{"location":"use_case_3/#pods-in-non-isolated-namespaces-should-be-able-to-talk","text":"We know that Pods can talk to Services in our default namespace - that's how our sample app works. But what about across namespaces? Since we're in a sandbox, we could use one of Pods in kube-system namespace to try reach Pods and Services in our app that's running in the non-isolated namespace default : # Get the name of one of the kube-system pods; tiller-deploy: src_pod=$(kubectl get pods --namespace kube-system | grep tiller | awk '{print $1}') # Now let's get an IP of the \"yelb-ui\" pod in \"default\" namespace: dst_pod_ip=$(kubectl get pods -o wide | grep yelb-ui | awk '{print $6}') # Now let's ask tiller-deploy to ping yelb-ui: kubectl exec --namespace kube-system -it ${src_pod} ping ${dst_pod_ip} The last command should result in output similar to: PING 10.47.255.246 (10.47.255.246): 56 data bytes 64 bytes from 10.47.255.246: seq=0 ttl=63 time=1.291 ms 64 bytes from 10.47.255.246: seq=1 ttl=63 time=0.576 ms Cancel the command with ^C. This confirms that pods in non-isolated namespaces can reach each other.","title":"Pods in non-isolated Namespaces should be able to talk"},{"location":"use_case_3/#isolated-pods-should-be-able-to-reach-non-isolated-services","text":"# Get the Cluster IP of the `yelb-ui` Service running in the `default` Namespace: default_yelb_ui_ip=$(kubectl get svc --namespace default -o wide | grep yelb-ui | awk '{print $3'}) # Get the name of \"yelb-appserver\" Pod in the \"dev-isolated\" Namespace: src_pod2=$(kubectl get pods --namespace dev-isolated | grep yelb-appserver | awk '{print $1}') # Run \"curl\" on \"yelb-appserver\" trying to reach the Service IP in \"default\" Namespace: kubectl exec -it -n dev-isolated ${src_pod2} -- /usr/bin/curl http://${default_yelb_ui_ip} We should see ~10 lines of HTML code of our main yelb-ui page, which shows that a Pod in dev-isolated Namespace can talk to a Service in non-isolated default Namespace.","title":"Isolated Pods should be able to reach non-isolated Services"},{"location":"use_case_3/#pods-in-an-isolated-namespace-should-not-be-reachable-from-other-namespaces","text":"Now, let's try to ping yelb-ui Pod that's running in the dev-isolated Namespace from the same tiller-deploy Pod: # Get an IP of the \"yelb-ui\" pod in \"dev-isolated\" namespace: isolated_pod_ip=$(kubectl get pods --namespace dev-isolated -o wide | grep yelb-ui | awk '{print $6}') # ..and try pinging it: kubectl exec --namespace kube-system -it ${src_pod} ping ${isolated_pod_ip} You should see that the command is \"stuck\", not displaying any responses because this time we're trying to reach something that isn't reachable because Tungsten Fabric is preventing it. Press ^C to cancel the command. Experiment a bit more on your own - try pinging isolated yelb Pods and Services from yelb Pods in default Namespace. Is everything working as you expected it to?","title":"Pods in an isolated Namespace should not be reachable from other Namespaces"},{"location":"use_case_3/#loadbalancer-services-in-isolated-namespaces-should-be-accessible-outside","text":"It wouldn't be much point to run application in an isolated Namespace if we couldn't access it, though. Therefore, our copy of yelb that is running in an isolated Namespace dev-isolated should be available to the Internet through the LoadBalancer Service yelb-ui . Let's test it: kubectl get svc --namespace dev-isolated -o wide | grep yelb-ui | awk '{print $4}' It should display something like afd9047c2915911e9b411026463a4a33-777914712.us-west-1.elb.amazonaws.com ; point your browser to it and see if our application loads!","title":"LoadBalancer Services in isolated Namespaces should be accessible outside"},{"location":"use_case_3/#cleanup","text":"Once you've explored enough, feel free to clean things up: # Delete the two copies of \"yelb\": kubectl delete -f cnawebapp-loadbalancer.yaml kubectl delete --namespace dev-isolated -f cnawebapp-loadbalancer.yaml # Delete the isolated namespace and its Manifest: kubectl delete -f dev-isolated.yaml rm -f dev-isolated.yaml","title":"Cleanup"},{"location":"use_case_3/#recap-and-whats-next","text":"Kubernetes Namespaces have been designed as a means of virtualising Kubernetes clusters. No virtualisation is complete without networking, and Tungsten Fabric's support for isolated Namespaces provides this function. Isolated Namespaces however have low granularity that you may need when implementing application network security policies within your Namespace. To get familiar with these more fine controls, check out the Use Case 4.","title":"Recap and what's next"},{"location":"use_case_4/","text":"Use case 4: Application micro-segmentation through Kubernetes Network Policies In most production environments there is a need to implement network access controls. Kubernetes provides a way to describe how groups of Pods should be able to communicate by using NetworkPolicy resources. As with most things in Kubernetes, for Network Policies to work, you need a Kubernetes CNI plugin that supports them. When would I care? Setting up explicit rules for what components of your application should be able to communicate is generally a good idea in almost all circumstances. Kubernetes Network Policy specification is a straight-forward way of doing that that lets you integrate NetworkPolicy definitions directly with your application manifest. The way NetworkPolicy resources are defined lets you be both precise in specifying what is network communications are permitted and what are not, and at the same time deal with the dynamic nature of applications running on Kubernetes by using podSelector definitions. This means your policies can target individual Pods or Pod groups, \"shrinking\" your security perimeter down to the size of a Pod. A tightly defined Network Policy combined with the default-deny configuration can save a lot of headaches from the effects of malicious application compromises, and/or misbehaving or misconfigured applications. For example, an application component may have a stuck cached DNS entry or a wrong configuration parameter causing it to talk to wrong backend. Or an application may get compromised and used as a beachhead to perform reconnaissance, attempt lateral movement, or simply use Pod's access to Kubernetes API to launch some crypto-miner pods to steal your compute resources. Securing our sample app with Network Policies The topic of Network Policy design is significantly larger than the space in this guide allows. In this example we will do the following: Create a default-deny Ingress policy for our default Namespace. This means all incoming connections to Pods running in this Namespace will have to explicitly allowed to work; and Create an Ingress NetworkPolicy objects for each of our sample application's components, allowing only what we are certain should be allowed. Step 1: figure out what should be able to talk to what First, we will need to remind ourselves how components of our application are expected to communicate. For that we'll go back to our app's diagram that we've seen in the introduction: From this diagram, we see that: The outside world will need to reach yelb-ui on TCP port 80 - (1) and (2) yelb-ui needs to reach yelb-appserver on TCP port 4567 yelb-appserver than in turn will need to reach yelb-db on TCP port 5432, and .. yelb-cache on TCP port 6379. Step 2: how do we identify our components? Remember that NetworkPolicy resources use Selectors to identify which Pods a policy applies to, and what are the source and destination of traffic that policy will govern. For this exercise, we will use podSelectror method, so we'll need to get a list of labels applied to our application Pods. Let's look through the cnawebapp-loadbalancer.yaml manifest for our sample app, and collect the labels: Pod Labels yelb-ui app: yelb-ui tier: frontend yelb-appserver app: yelb-appserver tier: middletier yelb-db app: yelb-db tier: backenddb redis-server app: redis-server tier: cache We are now ready to write our policies. These policies, once deployed, will control the communications between our app's components in the following manner: Step 3: A \"default-deny\" Policy Make sure you're on the sandbox control node, logged in as root, and in the correct directory: # Make sure we're root whoami | grep root || sudo -s # Change to the manifests directory cd /home/centos/yelb/deployments/platformdeployment/Kubernetes/yaml In this step, we'll create a Policy that will prevent all network communications that aren't explicitly allowed. We'll stick to only restricting Ingress traffic; but in practice you may want to do that for Egress as well (but watch out for blocking DNS queries when doing so!): cat > yelb-policy.yaml <<EOF # First, add Ingress default-deny # apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: default-deny spec: podSelector: {} policyTypes: - Ingress EOF This policy basically says: \"for any Pods, apply an Ingress Policy that has no rules\" which will result in all incoming traffic to all Pods in the Namespace where this Policy is applied to be dropped. Step 4: A Policy for \"yelb-ui\" The yelb-ui is slightly different from the other components in the sense that it's the only one that gets accessed from the outside, therefore its ingress: definition will use ipBlock of 0.0.0.0/0 to signify \"Everyone\": cat >> yelb-policy.yaml <<EOF --- # Policy to let anyone reach yelb-ui apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: yelb-ui-in spec: podSelector: matchLabels: app: yelb-ui tier: frontend policyTypes: - Ingress ingress: - from: - ipBlock: cidr: 0.0.0.0/0 ports: - protocol: TCP port: 80 EOF This Policy says: \"for Pods with Labels of app: yelb-ui and tier: frontend , allow incoming traffic from any source IP as long as it's going to the Pod's TCP port 80\". Step 5: Policies for the rest of Pods in our sample app The other 3 Pods in our sample app will only see traffic from other Pods, so their policies will use the podSelector parameter with labels of Pods that are allowed to send traffic: cat >> yelb-policy.yaml <<EOF --- # Policy to let yelb-ui reach yelb-appserver apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: yelb-appserver-in spec: podSelector: matchLabels: app: yelb-appserver tier: middletier policyTypes: - Ingress ingress: - from: - podSelector: matchLabels: app: yelb-ui tier: frontend ports: - protocol: TCP port: 4567 --- # Policy to let yelb-appserver reach yelb-db apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: yelb-db-in spec: podSelector: matchLabels: app: yelb-db tier: backenddb policyTypes: - Ingress ingress: - from: - podSelector: matchLabels: app: yelb-appserver tier: middletier ports: - protocol: TCP port: 5432 --- # Policy to let yelb-appserver reach redis-server apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: redis-server-in spec: podSelector: matchLabels: app: redis-server tier: cache policyTypes: - Ingress ingress: - from: - podSelector: matchLabels: app: yelb-appserver tier: middletier ports: - protocol: TCP port: 6379 EOF Step 6: Let's test how things work before applying the Policy So that we have a \"before and after\", let's deploy our sample app and capture a baseline: # Deploy our app kubectl create -f cnawebapp-loadbalancer.yaml Wait for the app to come up and become available externally: # Get an external DNS name of our app's yelb-ui Service: kubectl get svc -o wide | grep yelb-ui | awk '{print $4}' We should get back something like a0b8dfc14916811e9b411026463a4a33-1258487840.us-west-1.elb.amazonaws.com ; open it in your web browser; sample application should load. Next, we know that all Pod communications are unrestricted, so we should be able to ping from say yelb-ui to yelb-db - an activity that should not really happen when an application is running normally and we're not troubleshooting: # Get the full Pod name for \"yelb-ui\" src_pod=$(kubectl get pods | grep yelb-ui | awk '{print $1}') # Get an IP of the \"yelb-db\": db_pod_ip=$(kubectl get pods -o wide | grep yelb-db | awk '{print $6}') # Run a ping command from \"yelb-ui\" to \"yelb-db\": kubectl exec -it ${src_pod} ping ${db_pod_ip} We should see that the ping command is receiving responses; so there is an unrestricted network connectivity present. Press ^C to stop the command. Step 7: Deploying the policy and testing the results In this last step, we'll deploy our Policy and observe the effects: # Deploy the Network Policy kubectl create -f yelb-policy.yaml After running the command above, give it a few seconds to settle down. Tungsten Fabric will generate appropriate Security Groups in the background, and install them. Let's test if our ping command that worked just now still does: # Run a ping command from \"yelb-ui\" to \"yelb-db\" again: kubectl exec -it ${src_pod} ping ${db_pod_ip} This time, we see that there are no responses, since the communications are now blocked by the policy. Next, test if the app is still accessible from your web browser - it should be! Bonus step 8: explore Tungsten Fabric's Security Traffic Group visualisation Tungsten Fabric includes a function that visualised traffic flows in a \"project\" which in our case corresponds to the Kubernetes Namespace . To access it, follow the link on the Carbide Evaluation Page that you used to get access to the sandbox control node - at the top there is a link to Contrail UI , along with login and password . Follow the link, then in the top left corner click \"Monitor\" icon, followed by \"Security\" -> \"Traffic Groups\" in the menu. Then follow the crumb trail at the top and select \"k8s-default\" at the end of it: You should see a diagram similar to the below: Explore around; the flows that you see represent what our sample app does, including our unsuccessful attempt to ping from yelb-ui to yelb-db , and the fact that yelb-appserver makes outbound requests (which, if we go and investigate, turns out to be DNS queries for yelb-db ). Cleanup Once you've explored enough, feel free to clean things up: # Undeploy the Network Policy kubectl delete -f yelb-policy.yaml # Delete our sample app: kubectl delete -f cnawebapp-loadbalancer.yaml # Delete the Policy Manifest: rm -f yelb-policy.yaml Recap and what's next The ability to control application's network communications is crucial for many if not all production deployments. The way to implement such controls for applications running on Kubernetes is through NetwokPolicy resources; however for these resources to actually work you need a CNI plugin that supports them. Tungsten Fabric includes full support for NetworkPolicy , irrespective of where your Kubernetes cluster with Tungsten Fabric is running - in your private DC, or in a public cloud. Network Policies could be made very simple or very complex, and the best way to figure out what's best for your application is to dive deeper into use cases and examples on top of what we've provided. Here are a few resources that may help: Good detailed introduction to Network Policies A nice GitHub repository with many examples of well-documented Network Policy manifests A talk at 2019 Kubecon with a good section on strategies for Network Policies: https://youtu.be/_VNv7jBh1XA?t=1273 When dealing with controlling network traffic flows, we also need an ability to see them. We've briefly touched on some of the Tungsten Fabric's visibility tools in our Bonus stet 8 above, and will explore this area in more detail in our Use case 5.","title":"Use case 4"},{"location":"use_case_4/#use-case-4-application-micro-segmentation-through-kubernetes-network-policies","text":"In most production environments there is a need to implement network access controls. Kubernetes provides a way to describe how groups of Pods should be able to communicate by using NetworkPolicy resources. As with most things in Kubernetes, for Network Policies to work, you need a Kubernetes CNI plugin that supports them.","title":"Use case 4: Application micro-segmentation through Kubernetes Network Policies"},{"location":"use_case_4/#when-would-i-care","text":"Setting up explicit rules for what components of your application should be able to communicate is generally a good idea in almost all circumstances. Kubernetes Network Policy specification is a straight-forward way of doing that that lets you integrate NetworkPolicy definitions directly with your application manifest. The way NetworkPolicy resources are defined lets you be both precise in specifying what is network communications are permitted and what are not, and at the same time deal with the dynamic nature of applications running on Kubernetes by using podSelector definitions. This means your policies can target individual Pods or Pod groups, \"shrinking\" your security perimeter down to the size of a Pod. A tightly defined Network Policy combined with the default-deny configuration can save a lot of headaches from the effects of malicious application compromises, and/or misbehaving or misconfigured applications. For example, an application component may have a stuck cached DNS entry or a wrong configuration parameter causing it to talk to wrong backend. Or an application may get compromised and used as a beachhead to perform reconnaissance, attempt lateral movement, or simply use Pod's access to Kubernetes API to launch some crypto-miner pods to steal your compute resources.","title":"When would I care?"},{"location":"use_case_4/#securing-our-sample-app-with-network-policies","text":"The topic of Network Policy design is significantly larger than the space in this guide allows. In this example we will do the following: Create a default-deny Ingress policy for our default Namespace. This means all incoming connections to Pods running in this Namespace will have to explicitly allowed to work; and Create an Ingress NetworkPolicy objects for each of our sample application's components, allowing only what we are certain should be allowed.","title":"Securing our sample app with Network Policies"},{"location":"use_case_4/#step-1-figure-out-what-should-be-able-to-talk-to-what","text":"First, we will need to remind ourselves how components of our application are expected to communicate. For that we'll go back to our app's diagram that we've seen in the introduction: From this diagram, we see that: The outside world will need to reach yelb-ui on TCP port 80 - (1) and (2) yelb-ui needs to reach yelb-appserver on TCP port 4567 yelb-appserver than in turn will need to reach yelb-db on TCP port 5432, and .. yelb-cache on TCP port 6379.","title":"Step 1: figure out what should be able to talk to what"},{"location":"use_case_4/#step-2-how-do-we-identify-our-components","text":"Remember that NetworkPolicy resources use Selectors to identify which Pods a policy applies to, and what are the source and destination of traffic that policy will govern. For this exercise, we will use podSelectror method, so we'll need to get a list of labels applied to our application Pods. Let's look through the cnawebapp-loadbalancer.yaml manifest for our sample app, and collect the labels: Pod Labels yelb-ui app: yelb-ui tier: frontend yelb-appserver app: yelb-appserver tier: middletier yelb-db app: yelb-db tier: backenddb redis-server app: redis-server tier: cache We are now ready to write our policies. These policies, once deployed, will control the communications between our app's components in the following manner:","title":"Step 2: how do we identify our components?"},{"location":"use_case_4/#step-3-a-default-deny-policy","text":"Make sure you're on the sandbox control node, logged in as root, and in the correct directory: # Make sure we're root whoami | grep root || sudo -s # Change to the manifests directory cd /home/centos/yelb/deployments/platformdeployment/Kubernetes/yaml In this step, we'll create a Policy that will prevent all network communications that aren't explicitly allowed. We'll stick to only restricting Ingress traffic; but in practice you may want to do that for Egress as well (but watch out for blocking DNS queries when doing so!): cat > yelb-policy.yaml <<EOF # First, add Ingress default-deny # apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: default-deny spec: podSelector: {} policyTypes: - Ingress EOF This policy basically says: \"for any Pods, apply an Ingress Policy that has no rules\" which will result in all incoming traffic to all Pods in the Namespace where this Policy is applied to be dropped.","title":"Step 3: A \"default-deny\" Policy"},{"location":"use_case_4/#step-4-a-policy-for-yelb-ui","text":"The yelb-ui is slightly different from the other components in the sense that it's the only one that gets accessed from the outside, therefore its ingress: definition will use ipBlock of 0.0.0.0/0 to signify \"Everyone\": cat >> yelb-policy.yaml <<EOF --- # Policy to let anyone reach yelb-ui apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: yelb-ui-in spec: podSelector: matchLabels: app: yelb-ui tier: frontend policyTypes: - Ingress ingress: - from: - ipBlock: cidr: 0.0.0.0/0 ports: - protocol: TCP port: 80 EOF This Policy says: \"for Pods with Labels of app: yelb-ui and tier: frontend , allow incoming traffic from any source IP as long as it's going to the Pod's TCP port 80\".","title":"Step 4: A Policy for \"yelb-ui\""},{"location":"use_case_4/#step-5-policies-for-the-rest-of-pods-in-our-sample-app","text":"The other 3 Pods in our sample app will only see traffic from other Pods, so their policies will use the podSelector parameter with labels of Pods that are allowed to send traffic: cat >> yelb-policy.yaml <<EOF --- # Policy to let yelb-ui reach yelb-appserver apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: yelb-appserver-in spec: podSelector: matchLabels: app: yelb-appserver tier: middletier policyTypes: - Ingress ingress: - from: - podSelector: matchLabels: app: yelb-ui tier: frontend ports: - protocol: TCP port: 4567 --- # Policy to let yelb-appserver reach yelb-db apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: yelb-db-in spec: podSelector: matchLabels: app: yelb-db tier: backenddb policyTypes: - Ingress ingress: - from: - podSelector: matchLabels: app: yelb-appserver tier: middletier ports: - protocol: TCP port: 5432 --- # Policy to let yelb-appserver reach redis-server apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: redis-server-in spec: podSelector: matchLabels: app: redis-server tier: cache policyTypes: - Ingress ingress: - from: - podSelector: matchLabels: app: yelb-appserver tier: middletier ports: - protocol: TCP port: 6379 EOF","title":"Step 5: Policies for the rest of Pods in our sample app"},{"location":"use_case_4/#step-6-lets-test-how-things-work-before-applying-the-policy","text":"So that we have a \"before and after\", let's deploy our sample app and capture a baseline: # Deploy our app kubectl create -f cnawebapp-loadbalancer.yaml Wait for the app to come up and become available externally: # Get an external DNS name of our app's yelb-ui Service: kubectl get svc -o wide | grep yelb-ui | awk '{print $4}' We should get back something like a0b8dfc14916811e9b411026463a4a33-1258487840.us-west-1.elb.amazonaws.com ; open it in your web browser; sample application should load. Next, we know that all Pod communications are unrestricted, so we should be able to ping from say yelb-ui to yelb-db - an activity that should not really happen when an application is running normally and we're not troubleshooting: # Get the full Pod name for \"yelb-ui\" src_pod=$(kubectl get pods | grep yelb-ui | awk '{print $1}') # Get an IP of the \"yelb-db\": db_pod_ip=$(kubectl get pods -o wide | grep yelb-db | awk '{print $6}') # Run a ping command from \"yelb-ui\" to \"yelb-db\": kubectl exec -it ${src_pod} ping ${db_pod_ip} We should see that the ping command is receiving responses; so there is an unrestricted network connectivity present. Press ^C to stop the command.","title":"Step 6: Let's test how things work before applying the Policy"},{"location":"use_case_4/#step-7-deploying-the-policy-and-testing-the-results","text":"In this last step, we'll deploy our Policy and observe the effects: # Deploy the Network Policy kubectl create -f yelb-policy.yaml After running the command above, give it a few seconds to settle down. Tungsten Fabric will generate appropriate Security Groups in the background, and install them. Let's test if our ping command that worked just now still does: # Run a ping command from \"yelb-ui\" to \"yelb-db\" again: kubectl exec -it ${src_pod} ping ${db_pod_ip} This time, we see that there are no responses, since the communications are now blocked by the policy. Next, test if the app is still accessible from your web browser - it should be!","title":"Step 7: Deploying the policy and testing the results"},{"location":"use_case_4/#bonus-step-8-explore-tungsten-fabrics-security-traffic-group-visualisation","text":"Tungsten Fabric includes a function that visualised traffic flows in a \"project\" which in our case corresponds to the Kubernetes Namespace . To access it, follow the link on the Carbide Evaluation Page that you used to get access to the sandbox control node - at the top there is a link to Contrail UI , along with login and password . Follow the link, then in the top left corner click \"Monitor\" icon, followed by \"Security\" -> \"Traffic Groups\" in the menu. Then follow the crumb trail at the top and select \"k8s-default\" at the end of it: You should see a diagram similar to the below: Explore around; the flows that you see represent what our sample app does, including our unsuccessful attempt to ping from yelb-ui to yelb-db , and the fact that yelb-appserver makes outbound requests (which, if we go and investigate, turns out to be DNS queries for yelb-db ).","title":"Bonus step 8: explore Tungsten Fabric's Security Traffic Group visualisation"},{"location":"use_case_4/#cleanup","text":"Once you've explored enough, feel free to clean things up: # Undeploy the Network Policy kubectl delete -f yelb-policy.yaml # Delete our sample app: kubectl delete -f cnawebapp-loadbalancer.yaml # Delete the Policy Manifest: rm -f yelb-policy.yaml","title":"Cleanup"},{"location":"use_case_4/#recap-and-whats-next","text":"The ability to control application's network communications is crucial for many if not all production deployments. The way to implement such controls for applications running on Kubernetes is through NetwokPolicy resources; however for these resources to actually work you need a CNI plugin that supports them. Tungsten Fabric includes full support for NetworkPolicy , irrespective of where your Kubernetes cluster with Tungsten Fabric is running - in your private DC, or in a public cloud. Network Policies could be made very simple or very complex, and the best way to figure out what's best for your application is to dive deeper into use cases and examples on top of what we've provided. Here are a few resources that may help: Good detailed introduction to Network Policies A nice GitHub repository with many examples of well-documented Network Policy manifests A talk at 2019 Kubecon with a good section on strategies for Network Policies: https://youtu.be/_VNv7jBh1XA?t=1273 When dealing with controlling network traffic flows, we also need an ability to see them. We've briefly touched on some of the Tungsten Fabric's visibility tools in our Bonus stet 8 above, and will explore this area in more detail in our Use case 5.","title":"Recap and what's next"}]}